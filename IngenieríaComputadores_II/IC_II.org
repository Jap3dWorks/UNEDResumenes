#+title: Ingeniería Computadores II
#+startup: latexpreview content

* Clasificación arquitecturas paralelas
- Taxonomía Flynn
      - Computadores *SISD*, Un flujo de instrucciones y un flujo de datos.
      - Computadores *SIMD*, Un flujo de instrucciones procesa varios flujos de datos.
      - Computadores *MIMD*, Varios flujos de instrucciones y cada uno sobre un flujo de datos diferente.
            - Implementación *SPMD*, Simple program multiple data. sistemas con varias cpu,
              pueden ejecutar un mismo código sobre varios flujos de datos.
      - Computadores *MISD*, Varios flujos de instrucciones sobre un flujo de datos.

- Niveles de paralelismo funcional
      - *ILP*, instruction level paralelism, Las instrucciones de un programa se ejecutan en paralelo.
            - hardware o compilador, transparente al programador.
            - granularidad más fina
            - se mide en Instrucciones terminadas/retiradas por ciclo.
      - *Nivel de Bucle*, Se ejecutan en paralelo distintas iteraciones de un bucle.
      - *Nivel de funciones*, Procedimientos que constituyen un programa se ejecutan simultáneamente.
      - *Nivel de programas*, Programas que se ejecutan en paralelo.
      - Solo MIMD y MISD pueden implementar el paralelismo funcional.

- Que miden las medidas para evaluar el rendimiento
      - *Tiempo de respuesta*, tiempo en procesar una entrada.
      - *Productividad*, Entradas procesadas por unidad de tiempo.
      - *Funcionalidad*, Tipos de entrada diferentes que es capaz de procesar.
      - *Expansibilidad*, Capacidad de ampliar el procesamiento añadiendo bloques (modular).
      - *Escalabilidad*, Ampliar el sistema sin que suponga una devaluación.
      - *Eficiencia*, rendimiento / coste.

- Medidas de rendimiento
      - *CPI*, ciclos por instrucción.
            - $T_{CPU} = NI \cdot CPI \cdot T_{ciclo} = NI \cdot \left( \frac{CPI}{f} \right)$
                  - $T_{CPU}$, tiempo de cpu en una tarea.
                  - $NI$ número de instrucciones.
                  - $T_{ciclo}$, periodo de reloj del procesador.
            - $CPI = \frac{C_p}{NI}$
                  - $C_p$, ciclos de reloj de un programa o tarea.
            - $T_{CPU} = NI \cdot \left( \frac{CPE}{IPE} \right) \cdot T_{ciclo}$
                  - $CPE$, número medio de ciclos entre instrucciones.
                  - $IPE$, número medio de instrucciones que se emiten.
            - $T_{CPU} = \left( \frac{N_{operaciones}}{Op_{instruccion}} \right) \cdot CPI \cdot T_{ciclo}$
                  - $N_{operaciones}$, operaciones que realiza un programa.
                  - $Op_{instruccion}$, media de operaciones que puede codificar una instrucción.
      - *MIPS*, millones de instrucciones por segundo.
            - $MIPS = \frac{NI}{T_{CPU} \cdot 10^6}$
      - *MFLOPS*, millones de operaciones de coma flotante por segundo.
            - $MFLOPS = \frac{N_{op.coma.flot}}{T_{CPU} \cdot 10^6}$
            - $N_{op.coma.flot}$, Número de operaciones en coma flotante.
      - $S_p = \frac{rendimiento(p)}{rendimiento\_original}$
            - Speedup o ganancia

* Procesadores Segmentados
- *CISC*, Complex Instruction Set Computer, procesadores con un conjunto de instrucciones complejas
  con el fin de reducir el tamaño de los programas (al ser necesario menos instrucciones).
      - Una instrucción se compone de microinstrucciones.
      - En los 70s el tamaño de la memoria principal era un problema.
      - Dificulta el paralelismo entre instrucciones.
- *RISC*, Reduced Instruction Set Computer.
      - Fue posible cuando el tamaño de la memoria dejó de ser un problema.
      - Set reducido de instrucciones sencillas (similares a las microinstrucciones CISC).
      - Hardware de control simplificado.
      - Reduce el tamaño de la cpu.
      - Permite mejor paralelismo entre instrucciones (segmentación).
      - Como los operandos se cargan en registros reduce accesos a memoria.

** Arquitectura ASG
- Arquitectura Segmentada genérica (tipo RISC).
- | CO | OPd (destino) | OP1 (fuente 1) | OP2 (fuente 2) |
      - Instrucción genérica de 3 operandos
- *Etapas*
      - *IF* (instruction fetch), Lectura de la instrucción de la caché de instrucciones.
      - *ID* (Instruction Decoding), Decodificación de la instrucción y lectura de sus operandos del fichero de registros.
      - *EX* (Execution), Ejecución de las operaciones. También de las instrucciones de salto.
      - *MEM* (Memory acces), lecturas y escrituras a la caché de datos.
      - *WB* (Write-Back results), Escritura en fichero de registros.

- El tiempo de ejecución de una instrucción es el tiempo de la etapa más duradera.
      - $t_e = max\{t_{if}, t_{id}, t_{ex}, t_{mem}, t_{wb}\}$

- *Riesgos*
      - *Riesgos estructurales*, conflictos por los recursos. una instrucción no puede avanzar.
      - *Riesgos por dependencias de datos*
            - *RAW*, Read after write (dependencia verdadera), Se produce al leer un dato antes de que sea actualizado.
            - *WAR*, Write after read (antidependencia), se escribe un dato antes de que este sea leído.
            - *WAW*, Write after write (dependencia de salida), la instrucción última que debe escribir un dato no es la última (orden incorrecto).
      - *Riesgos de control*, instrucciones de control de flujo.

- *Forwargind* o *adelantamiento*. Conexión entre etapas *EX*, haciendo posible realizar un cálculo con el resultado de la instrucción anterior sin esperar a la etapa *WB*.

** Algoritmo Tomasulo
- Planificación de las instrucciones para evitar detenciones. El algoritmo Tomasulo es de las primeras
  soluciones para la planificación.
- El segmento *ID* se desdobla en *ID* y *II*.
      - *II* (Instruction Issue), La instrucción espera a que no hayan riesgos *RAW* y cuando estén todos los operandos fuente listos emite la instrucción a la unidad funcional.
- *RS*, estaciones de reserva, reservation stations, las instrucciones esperan aquí para ser emitidas hacia la unidad funcional.
      - Instrucciones ya decodificadas
      - Por cada operando contiene 2 campos, *valor* y *etiqueta*.
      - Se forma una pequeña cola de instrucciones.
      - Cada unidad funcional suele tener una *RS*.

*** Funcionamiento con FPU IBM 360/91
1. Las instrucciones se emiten desde la *FLOS* a la *RS*.
       - Se copian operandos disponibles y se marca el bit *ocupado* (dato anticuado) del registro destino en el *FR*.
       - Se copia el valor etiqueta del registro destino en el *FR* que identifica a la *RS* donde se envía la instrucción.
       - Si el operando no está disponible (ocupado=1) se sustituye por la *etiqueta* que hay en *FR*.
       - Los registros destino del *FR* se marcan como ocupados (actualización pendiente).
       - Esta etiqueta contiene el origen del dato cuando esté disponible.
             - La *etiqueta* puede ser una de las instrucciones en reserva *RS* o uno de los registros del *FB*.
2. Los datos de la unidad funcional se publican junto con su etiqueta en el *CDB*.
       - La *RS* está constantemente comprobando las publicaciones en la *CDB*.
3. Si la *RS* encuentra el contenido de una etiqueta en la *CDB* la copia en la instrucción reservada y la marca como
   preparada.
4. Instrucciones sin dependencias se envían a la unidad funcional.

[[file:images/UCFlotanteConTomasulo.png]]

* Procesador Superescalar
- Varias instrucciones avanzar simultáneamente por el cauce.
      - varias unidades funcionales.
- Ejecución de instrucciones en un orden diferente a lo especificado en el programa (fuera de orden).
- Hay renombramiento de registros.
      - Para eliminar dependencias falsas.
  
** Arquitectura
*** Segmentos Etapas
- *IF* (instruction fetch), Lectura de la instrucción de la caché de instrucciones.
- *ID* (Instruction Decoding), Decodificación de la instrucción y lectura de sus operandos del fichero de registros.
- *II* (Instruction Issue), Estados de *distribución/emisión*.
- *EX* (Execution), Ejecución de las operaciones. También de las instrucciones de salto.
- *WR* (write back results), escritura en fichero de registros.
- *RI* (Retirement instruction), etapa de retirada.

*** Estados de una Instrucción
- *Distribuida* (dispatched), ha sido enviada a una estación de reserva (*RS*) asociada a una o varias
  unidades funcionales del mismo tipo.
- *Emitida* (issued), sale de una *RS* hacia una unidad funcional.
- *Finalizada* (finished), abandona la unidad funcional y va al buffer de reordenamiento, los resultados no son accesibles al programador (registros de renombramiento).
- *Terminada* (completed), completada la escritura desde los registros de renombramiento hasta los registros arquetectónicos (visibles al programador).
- *Retirada* (retired), ha realizado la escritura en memoria.

*** Diagrama Segmentación Superescalar genérica
[[file:images/SuperescalarGenérica.png]]

*** Problemas
- *Falta de alineamiento*, 
- *Rotura de secuencialidad*, 


** Lectura de instrucciones
- *Grupos de lectura*, Instrucciones que se extraen de la i-caché en un ciclo.

*** Falta de alineamiento
- Alineamiento incorrecto, las instrucciones de un grupo de lectura exceden el tamaño con el bloque de lectura del procesador.

*** Rotura de secuencialidad
- Una de las instrucciones del grupo lectura es un salto o una rotura de secuencialidad.
- *Coste de oportunidad* coste derivado de no aprovechar bien el cauce de instrucciones, debido a saltos y saltos condicionales.
- Es importante la detección temprana de las instrucciones de salto.
      - Lo habitual es en el *ID*. Cuanto antes mejor para poder analizarlas.

*** Predicción dinámica
**** Tratamiento
- Cómputo de los saltos suele ser una unidad funcinal dedicada.
- *Especular*, intentar predecir el resultado de un salto
      - Se intenta no detener el cauce hasta saber el resultado del salto.
- *Predicción estática*, el compilador deduce el resultado de una condición.
  Por tanto no es necesaria una instrucción de salto condicional.

**** Predicción de la dirección de destino de salto mediante BTAC
- *Branch Target Adress Cache* (BTAC), Memoria cache asiciativa que almacena las direcciones de los saltos efectivos ya ejecutados.
      - Columnas de la BTAC
            - *BIA*, branch instruction address, no almacena la instrucción completa, por rendimiento.
            - *BTA*, branch target address.
- *Falso positivo*, Distintas direcciones pueden dar el mismo valor de BTAC.
- Se usa el PC para saber si la instrucción en la posición PC está en la BTAC.
- Si está, en ese PC ha habido salto anteriormente.
- Si el PC no está y es un salto se considera el salto no efectivo.
- Si finalmente el salto es efectivo (etapa EX) se actualiza la BTAC.
- Si el salto no es efectivo se elimina la entrada de la BTAC si la hay.
- Si no hay entradas libres y tenemos que añadir una nueva entrada se puede usar un algoritmo tipo *LRU*.

**** Predicción de la dirección de destino de salto mediante BTB con historial de salto
- *Branch target Buffer*, Similar a BTAC pero además se almacena un conjunto de bits con el historial de aciertos (campo BH).
      - El historial sirve para realizar un reemplazo si es preciso.

**** Predicción de Smith o predictor bimodal
- Es un tipo de *BTB*.
- Campo *BH* como contador de saturación, una máquina de estados.
      - 00, Strongly not taken.
      - 01, Weakly not taken.
      - 10, Weakly taken.
      - 11, Strongly taken

**** Predictor de dos niveles basado en historial global
- *BHR*, Branch History Register, historial global de saltos.
      - Registro de desplazamiento de tamaño $h$, se introducen los nuevos valores por la derecha.
      - 1 en caso de salto 0 no hay salto.
- *PHT*, Pattern history table.
      - Entradas de tamaño $h+m$, los $m$ bits se extran con una función hash sobre la dirección de la instrucción.
      - Cada entrada tiene un contados de saturación de 2 bits.
            - Actua como el predictor bimodal.
        
**** Predictor de dos niveles basado en el historial local
- Muy similar al *predictor de dos niveles basado en historial gobal*
- *BHT*, Branch History Table
      - Ahora el historial de saltos es local.
      - Se usa una función hash sobre la dirección de la instrucción de salto (se reduce hasta $k$ bits) para acceder a la entrada de la *BHT*.
      - $2^k$ entradas en la *BHT*.
- Acceso a la *PTH.*
      - hash sobre la dirección de la instrucción
      - Valor obtenido de la *BHT*
      - Se combinan y se accede al contador de saturación.

**** Predictos de dos niveles con indice compartido gshare
- Variante del predictor de dos niveles con historial global
- 
  [[file:images/PredictorGShare.png]]

**** Predictores híbridos
- Procesadores superescalares actuales combinan dos predictores y un selector.

** Distribución
*** Buffer de distribución
- También se conoce como *ventana de instrucciones*.
- Las instrucciones se incluyen tras la decodificación.
- Se busca desacoplar la etapa de decodificación de la etapa de ejecución.
- La instrucción se deposita junto a los identificadores de los operandos fuentes.
      - 1 bit por operando indica si este está disponible.
#      - Las instrucciones esperan a tener los operandos disponibles.
- Las *estaciones de reserva* (RS) almacenan las instrucciones decodificadas.
- *distribución*, instrucción a la estación de reserva (vinculada a una unidad funcional).
- *emisión*, instrucción a la unidad funcional.

*** Organización del Buffer de distribución
- *Estación de reserva centralizada*, equivale a un buffer de distribución que actua como estación de reserva global.
- *Estación de reserva distribuidas o individuales*
      - Cada unidad funcional dispone de una estación de reserva propia.
      - Un *buffer de distribución* reparte las instrucciones entre las estaciones de reserva.
- *Estaciones de reserva en clústers o compartidas*
      - Las estaciones de reserva reciben las instrucciones del *buffer de distribución*.
      - Una estación de reserva puede servir a varias unidades funcionales.

*** Operativa de una estación de reserva individual
- Campos Generales
      - *Ocupado (O)*, Entrada ocupada por una instrucción pendiente de emisión.
      - *Código de operación (CO)*, El código de la operación.
      - *Operando 1 (OP1)*
            - Si el registro está disponible almacena el valor o el identificador (depende del diseño).
            - Si no está disponible contiene el identificador del registro.
      - *Válido (V1)*, Indica si el operando fuente está disponible.
      - *Operando 2 (OP2)*
      - *Válido (V2)*
      - *Destino (D)*, Identificador del registro destino que almacenará el valor de forma temporal (renombramiento de registros).
      - *Listo (L)*, Todos los operandos están disponibles y puede emitirse.
        
*** Fase de distribución
- Envío de una instrucción desde el buffer de distribución a la estación de reserva.
- Se distribuyen en orden al *buffer de distribución*.
- Si la *lectura de operandos* se realiza en esta fase, Los identificadores de los operandos disponibles se sustituyen por su valor en la *estación de reserva*.

*** Fase de supervisión
- La instrucción ya está en la estación de reserva, se espera a que los dos *operandos fuente* estén disponibles.
- Las instrucciones con operandos fuente pendientes se encuentran es espera activa.
      - Revisan el contenido del *CDB* en espera de sus operandos.
      - *Lógica de activación*, hardware que realiza la supervisión de los buses.

*** Fase de emisión  
- Instrucciones con todos sus operandos fuente disponibles.
- Espera activa de *lógica de selección*.
      - Determina que instrucción se puede emitir.
- Da paso a la *ejecución*, bit ocupado=0 en estación de reserva.
- *Planificacior dinámico*, selecciona y emite una instrucción de las listas para emitir.
      - Es habitual seleccionar la instrucción más antigua.
- *Emisión alineada*, El *buffer de distribución* no puede enviar nuevas instrucciones hasta que la *estación de reserva* esté vacía.
- *Emisión no alineada*, Sí que puede distribuir instrucciones mientras haya huecos.

*** Lectura  de operandos
- *Planificación sin lectura de operandos*, La lectura de los operandos fuente se realiza al emitir desde la *estación de reserva* a la *unidad funcional*.
      - Cuando las instrucciones de distribuyen desde *el bufer de distribución* a la *estación de reserva* se analizan las dependencias verdaderas.
      - Si hay dependencias se marca el registro como inválido en el *fichero de registros*.
      - Registros que son el destino de una operación.
      - Al acabar la operación la unidad funcional coloca el identificador del registro destino y el valor en los buses de distribución.
- *Planificación con lectura de operandos*, La lectura de operandos se efectua en la *distribución* (buffer distribución -> estación de reserva).
      - Código operación e identificador de registro destino se copian directamente a la estación de reserva.
      - Si los operandos fuente están disponibles se copia el valor y se pone el bit de validez a 1.
      - Si no se copia el identificador del registro y el bit de validez a 0.
      - La espera activa revisa el CDB a la espera de los operandos pendientes.

*** Renombramiento Registros
- los registros de renombramiento se suelen anotar como *Rr<número>*.
- *RRF*, Rename register file, Buffer de renombramiento o fichero de registros de renombramiento.
      - Aquí se guardan los registros de renombramiento.
- *ARF*, Achitected register file, fichero de registros arquitectónicos (visibles al programador).
- *Buffer Renombramiento*, [[Buffer de renombramiento][Buffer Renombramiento]] .
  
*** Buffer de renombramiento
- Estructura que mantiene entradas con las instrucciones que hay al vuelo.
- Decide cuando los datos en *RRF* deben escribirse en el *ARF*.
- Entradas:
      - *Ocupada* (O),
      - *Emitida* (E),
      - *Finalizada* (F),
      - *Dirección* (Dir),
      - *Registro de destino* (Rd),
      - *Registro de renombramiento* (Rr)
      - *Especulativa* (Es),
      - *Validez* (V),

*** Organización independiente del RRF con acceso indexado

*** Organización independiente RRF con acceso asociativo

*** Organización del RRF como parte del buffer de renombramiento

** Instrucciones Terminadas
*** Buffer de reordenamiento o terminación
- Las instrucciones se incluyen tras la *decodificación* en orden.
- Asegura la *consistencia de procesador*, Las instrucciones concluyen el procesamiento en el mismo orden secuencial que el programa.
      - Garantiza el resultado final del programa.
            - Actualización correcta desde el *RRF* al *ARF*.
      - Permite un tratamiento correcto de las interrupciones.
            - Puede expulsar del cauce todas las instrucciones posteriores a la interrupción.
- *Campos*
      - *Ocupada (O)*, 1 bit, indica que la instrucción se ha distribuido. Permanece en 1 hasta que es terminada.
      - *Emitida (E)*, 1 bit, vale 1 cuando la instrucción inicia su ejecución en la unidad funcional.
      - *Finalizada (F)*, 1 bit, ha salido de la unidad funcional, espera ser terminada arquitectónicamente.
      - *Dirección (Dir)*, dir en memoria de la instrucción para identificarla.
      - *Registro destino (Rd)*, Identificador del registro destino, se actualiza una vez esté *Terminada* la instrucción.
      - *Registro de renombramiento (Rr)*, Identificador del registro de renombramiento asociado a la instrucción.
        Así se sabe que registro del *RRF* hay que liberar.
      - *Especulativa (Es)*, 1 bit, identifica la instrucción como parte de una ruta especulativa.
        Mientras sea especulativa no puede terminar.
      - *Validez (V)*, 1 bit, indica si puede terminarse o por el contrario se ignora al terminarse.
            - e.g especulativa con predicción errónea.

- Tiene estructura circular de anillo con un puntero en la cabeza y un puntero en la cola.
      - Las instrucciones de registran en el orden del programa.
            - Se avanza la cabeza cuando se escribe una nueva instrucción.
            - Se avanza la cola cuando se termina una instrucción.
            - La cabeza no puede sobrepasar la cola ni viceversa.

** Instrucciones Retiradas
- Solo instrucciones de almacenamiento en memoria.
*** Terminación ordenada en buffer de almacenamiento
- Instrucciones *Retiradas*.
- Escritura en memoria coherente.
      - Evitar riesgos WAW y WAR en memoria.
- Cuando la instrucción es terminada se marca para proceder a la escritura en memoria.
      - Cuando el bus esté libre.
- A una misma dirección se escribirá aquella instrucción posterior en el código del programa.
  
** Lectura de operandos
- *Planificación sin lectura de operandos*, Lectura de operandos en la emisión de instrucciones desde las estaciones de reserva individuales a las unidades funcionales.
- *Planificación con lectura de operandos*, 

* Procesadores VLIW
** VLIW
- 1 instrucción es una concatenación de varias instrucciones tipo *RISC* que se pueden ejecutar en paralelo.
- Operaciones recogidas dentro de una instrucción no representan dependencia de datos.
      - Le permite procesamiento en paralelo en cada emisión.
      - Las macroinstrucciones creo que las define el compilador, no están preescritas.
- Tamaño de instrucción más habitual son 256 bits.
      - En cada celda se puede escribir una instrucción RISC.
      - | Salto |Carga/almac | Carga/almacen | Entera | Entera | float | float | Util |
- La responsabilidad no es del Hardware, sino del compilador.
      - Sigue habiendo segmentación a nivel de instrucción RISC.
      - El hardware se limita a emitir una instrucción por ciclo.
      - La *planificación es estática*.
            - *Latencia*, ciclos que tarda un resultado en estar listo.
                  - Notese que pueden seguir mandándose instrucciones a la unidad fucional aunque el resultado anterior aún no esté resuelto.
      - VLIW es código de baja densidad.
            - No siempre se puede hacer una planificación óptima y hay relleno (nop) entre instrucciones.
      - El código compilado no es compatible con otras generaciones de procesadores VLIW.

** Planificación VLIW
- Tareas que requiere el compilador
      - *Código Intermedio*,  Sencillas instrucciones RISC.
            - Solo dependencias verdaderas (RAW).
                  - sin WAW ni WAR ya que el compilador dispone de infinitos registros simbólicos.
            - Principal dificultad para el compilador es agrupar instrucciones RISC en una instrucción VLIW.
      - *Grafo control de flujo*, detección de bloques básicos.
            - Bloque básico, secuencia de instrucciones que forma una ejecución secuencial.
                  - Se detectan observando las etiquetas y las directrices de salto.
      - *Grafo del flujo de datos*.
            - *Planificación local*, En Cada bloque se pueden buscar instrucciones VLIW.
                  - El paralelismo queda un poco limitado.
                  - *Desenrollamiento de bucles*, Expandir el bucle a una secuencia de instrucciones sin salto.
                        - Proporciona una mejor capacidad de planificaciń al compilador.
                  - *Segmentación software*,
                        - Producir un uevo bucle intercalando instrucciones de diferentes iteraciones.
            - *Planificación global*, Se combinan instrucciones de varios bloques.
                  - *Planificación de trazas*,
                        - Manipular diferences bloques básicos para crear bloques de mayor tamaño.
                        - *Traza*, representa el camino de ejecución más probable.
                        - *Selección de la traza*, bloques básicos que forman una secuencia sin bloques.
                              - Especulación de los caminos más probables.
                              - Se recurre a un grafo de flujo ponderado.
                        - *Compactación de la traza*, generación del código VLIW minimizando el número de operaciones vacías.
                              - Se benefician las rutas más probables.
                              - Consideraciones del compilador
                                    - Cual es la secuencia más probable?
                                    - Conocer las dependencias de datos
                                    - Que cantidad de código de compensación es necesario.
                                    - Medir el coste ahorrado del desplazamiento de operaciones.
                                          - Medido en ciclos de ejecución y en coste de memoria.
            - Muestra las instrucciones sin dependencia entre ellas.

** Procesamiento especulativo
- Técnica basada en ejecutar instrucciones antes de saber si el programa irá por ese cauce.
  Si la especulación es erronea se descartan los resultados.
- Se usa cuando sabemos la posibilidad de cada bifurcación.
- El compilador es el responsable de colocar las instrucciones y registros de forma que se anulen o sobreescriban entre las bifurcaciones.

** Instrucciones con Predicado
- También se conocen como operaciones condicionadas o operaciones con guardia.
- El predicado determina si almacena o se descarta el resultado de una instrucción.
      - Se quiere reducir el número de saltos condicionales.
- <instrucción> (P)
      - p=1 almacena, p=0 descarta
- Los predicados tienen registros especiales p<dígito>.
- Hay instrucciones para escribir en los registros de predicado.

#+NAME: formato predicados
#+CAPTION: Formato predicados
#+BEGIN_SRC asm
    PRED_CLEAR p1,p2           ; p1:=false, p2:=false
    PRED_EQ p1,p2,reg,valor    ; p1:=(reg==valor), p2:=NOT(p1)
    PRED_NE p1,p2,reg,valor    ; p1:=(reg!=valor), p2:=NOT(p1)
    PRED_LT p1,p2,reg,valor    ; p1:=(reg<valor), p2:=NOT(p1)
    PRED_GT p1,p2,reg,valor    ; p1:=(reg>valor), p2:=NOT(p1)
#+END_SRC

** Tratamiento de excepciones
- Para ganar paralelismo se recurre a reordenar instrucciones.
- *Centinelas*, similar al buffer de reordenamiento superescalar.
      - El compilador marca las instrucciones VLIW (reordenadas) especulaticas con una etiqueta.
      - El lugar del programa donde estaba la instrucción se marca con un *Centinela*
      - La instrucción deja de ser especulativa cuando se ejecuta el *centinela*.
            - Ya es posible escribir en registros arquitectónicos o en memoria.
- Se puede implementar con un buffer de terminación.
      - Instrucciones especulativas se retiran cuando se ejecute el *centinela* asociado.

* Procesacores Vectoriales
** Arquitectura básica
- SIMD
- *Unidad procesamiento Vectorial*.
      - *Fichero de registros vectoriales*.
            - Conjunto de registros vectoriales, entre 64-256.
            - número de elementos por registro está entre 8-64
                  - *MVL*, máximmum Vector Length. tamaño de las operaciones vectoriales.
            - Cada Elemento de un registro suele ser de 8bytes.
      - *Unidades funcionales vectoriales*.
            - Segmentadas, pueden iniciar una operación cada ciclo de reloj.
            - *lanes* o carriles, $n$ unidades funcionales en paralelo (carriles), acceleran en un factor $n$ el cálculo.
      - *Unidad carga/almacenamiento vectorial*.
            - Segmentada, tras la latencia realiza 1 operación por ciclo de reloj.
- Unidad procesamiento escalar, un procesador escalar segmentado común.
- *Procesadores Matriciales* son muy parecidos a los vectoriales.
      - Constan de $n$ *EPs* (elementos de procesamiento) fuertemente interconectados.
            - Cada EPs consta de una ALU, conjunto de registros,
              una unidad de carga/almacenamiento, memoria local (*MEPs*), etc.
      - En un ciclo puede computar $n$ elementos.

** Medida del rendimiento procesadores Vectoriales
- $T_n = T_{arranque} + n * T_{elemento}$
      - Tiempo en procesar $n$ elementos.
      - Tiempo de arranque de cada módulo es similar al número de segmentos del que consta.
- *Convoy*, Varias instrucciones sin dependencias ni riesgo estructurales planificadas.
      - Puede resolverse en paralelo porque no existe riesgo estructural.
      - Usan diferentes módulos del procesador vectorial.
- *FLOPs*, $R_n = (Op_{f} \times n) / T_n$
      - operaciones de coma flotante por segundo.
      - $n$, número de elementos del vector.
      - $Op_{f}$, operaciones de coma flotante por elemento,
        si por ejemplo el convoy tiene dos instrucciones aritméticas $Op_{f}=2$
- *Chainning*, Es como el adelantamiento (forwarding) entre unidades funcionales vectoriales.
      - Permite la existencia de dependencias dentro de un *Convoy*.

** Carga/almacenamiento
- *bancos de memoria*, unidades de memoria en paralelo.
      - Permiten el almacenamiento o carga a 1 ciclo por elemento
      - Lo ideal $T_{a} = m$
      - $m$ bancos de memoria como $T_a$ ciclos que cuesta el acceso a memoria.
- Mientras se hace una petición de acceso, cada banco de memoria realiza su operación.
      - Se puede ver como una especie de segmentación.
- $T_{op} = T_a + n$
      - Tiempo de una operación es igual al tiempo de acceso más 1 ciclo por elemento en el vector.
- El alineamiento suele ser a 8 bytes (vectores trabajan con doubles).
      - elemento en dirección ###000###, irá al banco 0.
      - elemento en dirección ###101###, irá al banco 5.

* Procesamiento Paralelo
** Organización basda en el modelo de comunicación
*** Espacio Direcciones Compartido
- Memoria compartida a través de la red de interconexión (pueden ser varios módulos).
      - *UMA*, Uniform memory access, tiempo de acceso a memoria es el mismo para cualquier palabrad desde cualquier procesador.
      - *Latencia de red*, Tiempo que tarda en enviar un mensaje a través de la red de interconexión.
      - *Ancho de bamda*, número de bits por unidad de tiempo (e.g bps).
      - *Bus* como arquitectura más común (sistemas pequeños).
      - Tiene problemas de escalabilidad, accesos que se chocan.
- *NUMA*, Non Uniform Memory Access, como UMA con pequeñas memorias por procesador donde se incluye el código que está ejecutando.
- *ccNUMA*, cache-coherent NUMA, Sistema para mantener la coherencia de las caches de cada procesador.
      - Cada nodo contiene una porción de la memoria total del sistema (no hay una memoria global).
            - Cada nodo consta de uno o varios procesadores (y sus cachés) y una memoria principal.
            - Para mantener la coherencia de las caches se usan los /protocolos snoopy/ o manteniendo un registro de las variables de cada procesador (directorios).
      - Memorias conectadas por la red de interconexión.
- *COMA*, Cache-Only Memory Access, para mantener la coherencia de las caches de cada procesador.
      - Nodos sin memoria local, tan solo cache.
            - Memorias locales a actuan como cache.
            - Los nodos hacen copias de los datos a sus memorias cache locales.
            - Ventaja, tratar los fallos de acceso al distribuir los datos por el sistema.
            - Desventaja, muy dificil mantener la coherencia.

*** Paso de mensajes

** Sistemas de memoria compartida
*** Redes de Interconexión
**** Redes Estáticas
- Topología definida durante la construcción de la máquina.

***** *Topología Red Unidimensional*
- *Red Lineal*, Conectar cada procesador a dos procesadores vecinos, extremos abiertos.
- *Red Anillo*, Colo la Red Lineal pero cerrada.

***** *Topología Red bidimensional*
- *Red Anillo Cordal*, Red Anillo con conexión cada X nodos.
- *Red Malla*, Malla de cuadrados.
      - *Mesh Cuadrada*, mismo número nodos en X e Y.
      - Si se conectan los extremos tenemos una *Mesh cerrada* o Toro.
- *Red Sistólica*, Red Malla con la diagonal conectada
- *Red Completamente conectada*, anillo donde todos los nodos tienen una conexión con cualquier otro nodo.
- *Red Estrella*, Un procesador central conectado a los demás procesadores.
- *Redes de árbol*, A veces solo las hojas son procesadores y los demás nodos son elementos de conmutación.
  Solo un camino une dos nodos en una red de árbol.
      - Desventaja cuando muchos nodos de un lado quieren comunicarse con los nodos del lado contrario.
        Las raices pueden verse saturadas.
      - Árbol Fat Tree, para aliviar la saturación puede aumentarse el número de conexiones en los conmutadores
        más cercanos a la raiz (es un camino de mayor tránsido).

***** *Topología Red Tridimensional*
- *Mesh tridimensional*,
        
***** *Topología Red Hipercubos*
- hipercubo dimensión $d$ esta compuesto por $p=2^d$ procesadores.
- Cada procesador se etiqueta con un número en binario de $d$ dígitos.
- Dos procesadores se conectan entre sí si las etiquetas difieren de un único dígito en binario.
- Un procesador de un hypercubo de dimensión d se conecta directamente a otros d procesadores.
- Un hypercubo de dimensión $d$ puede dividirse en 2 hipercubos de dimensión $d-1$.
      - Cada partición son todos los procesadores con el mismo valor en un mismo dígito de su etiqueta.
- *Distancia de Hamming*, número total de posiciones de bits de dos procesadores para los que sus etiquetas son diferentes.
      - Es la suma de todos los unos de el resultado de la operación xor entre las etiquetas.
      - $\sum \{ n\ |\ \forall n \in e_a \oplus e_b \}$
      - El camino de encuentra aplicando las diferencias entre las equiquetas desde el bit menos significativo.

[[file:images/RedHipercubo.png]]        

**** Redes Dinámicas
- Sistemas paralelos de propósito general.
- *Redes basadas en Bus*, los nodos comparten un único medio de comunicación, el Bus.
      - En un instante solo un nodo puede transmitir por el Bus.
      - Lógica de arbitraje para prevenir colisiones
            - Prioridad fija, FIFO, Round Robin, LRU (last recently used).
      - Puede mejorarse el rendimiento con memorias cache locales.
- *Redes Crossbar*, conecta $p$ procesadores con $q$ elementos de memoria con una red de conmutadores.
      - Conmutadores necesarios $p \times q$.
      - Cantidad de conmutadores dificil de escalar.
      - tipo no-bloquetante, procesador A accediendo a memoria A no interfiere con procesador B accediendo a memoria B.
- *Redes Multietapa*, Se componen de etapas (elementos lógicos) conectadas con otras etapas mediante conmutadores.
      - Compromiso entre el Bus y el Crossbar.
      - Redes bloqueantes.
      - *Conmutador* $a \times b$, dispositivo con $a$ entradas y $b$ salidas.
            - Sus configuraciones permiten alternan las conexiones de las entradas con las salidas.
            - Una entrada puede estar conectada a varias salidas pero no al contrario.
            - Conmutación dinámica.
      - *Red Omega*, permutación por /barajamiento perfecto/ (perfect suffle) bien hacia la derecha o hacia la izquierda.
            - Se conectan $p$ procesadores con $p$ memorias.
            - Hay $log_2p$ etapas.
            - Conmutadores de $2 \times 2$.
            - Se necesitan $\frac{p}{2}log_2p$ conmutadores.
            - Con la dirección de memoria en binario, desde un procesador.
                  - El bit más significativo aún no enrutado.
                  - bit 0 salida superior. bit 1 salida inferior.
                  - La interconexión de conmutadores (etapas) sigue el barajamiento perfecto.
      - *Red Baseline*, Se construye por bloques conectando las salidas del los conmutadores $n$ con las entradas de los conmutadores $n+1$.
            - En cada bloque se conectan por barajamiento perfecto.
            - En la siguiente etapa $n_{i+1}$, los bloques se dividen a la mitad ($n_{i}/2$ entradas $\times$ $n_{i}/2$ salidas)
              [[file:images/RedBaseline.svg]]
      - *Red Butterfly*,
            - Entre conmutadores existe la conexión directa o cruzada.
            - Cada conmutador se identifica como $[i,j]$ donde $i$ es la etapa y $j$ la fila de conmutadores.
            - El conmutador $[i,j]$ conecta con $[i+1,j]$ y $[i+1, j \oplus 2^i]$.
              [[file:images/ButteflyConn.png]]
      - Desde el conmutador origen $A$ hasta el conmutador destino $B$ la ruta $R$ se calcula.
            - Notese que $j$ son los dígitos en común de las dos conexiones de entrada originales.
                  - Pocesador 000 y 001 conectan con conmutador 00.
            - $R=A \oplus B$, desde el bit menos significativo $R_0$.
                  - Si $R_i=1$ camino cruzado, Si $R_i=0$ camino directo.
      - En notación de permutación ($i$ es el índice de los conmutadores destino).
        
         $\beta_i(x_{n-1}\ \ldots \ x_{i+1} \ x_i \ x_{i-1}\ \ldots \ x_1\ x_0) = x_{n-1} \ \ldots \ x_{i+1} \ x_0 \ x_{i-1} \ \ldots \ x_1\ x_i$
         [[file:images/ButterflyPerm.png]]

*** Protocolos Coherencia de Caché
- *Snoopy*, vigilancia del bus.
      - Común en bus o en anillos.
      - Cada procesador monitoriza el tráfico de la red en busca de transacciones.
      - Los bloques de cache tienen etiquetas, para marcar si un dato está desactualizado o sucio (el procesador lo ha manipulado).
            - Caches locales y memoria principal compartida.
            - Si otro procesador (B) escribe en un bloque donde se mantiene una copia en cache en el procesador A, esta se marca como inválida en A.
            - Si el procesador A escribe en un bloque en su mem caché lo marca como sucio.
                  - Cuando otro procesador (B) necesite este bloque, el procesador A toma el control del bus y lo actualiza en memoria.
            - Al escuchar el tráfico puede o bien invalidar su copia o notificar a otro procesador que solicita un dato el nuevo valor.
      - Consume mucho ancho de banda.
- *Systema basado en directorios*, 

* Glosario
- *D-Caché*, Caché de datos del procesador.
- *I-Caché*, Caché de instrucciones del procesador.
- *Unidad Funcional*, módulos de cómputo en un procesador, pueden funcionar de forma independiente.
- *Planificación Dinámica*, el hardware reorganiza la ejecución de instrucciones par reducir las detenciones.
- *FPU*, floating point unit.
      - *FR*, Floating Register.
            - los registros tienen un campo *ocupado* y un campo *etiqueta* (planificación).
      - *FB*, Floating Buffer, aquí entran los datos desde memoria.
      - *SDB*, Store Data Buffers, de aquí los datos se escriben en memoria.
            - Puede ser el destino de instrucciones, por tanto los registros del SDB tienen un campo *etiqueta*.
      - *FLOS*, floating point operation stack, pila de operaciones de coma flotante.
- *CDB*, Common data bus, permite cargar un resultado en todas las unidades funcionales, conecta salida de unidades funcionales con las *RS*, *FR* y *SDB* (adelantamiento).
      - Las unidades funcionales publican aquí sus resultados.
- *Acciones Atómicas*, Acciones indivisibles en subacciones.
- *Barajamiento Perfecto*, perfect suffle.
      - $\sigma^k(x_{m-1}\ x_{m-2}\ \ldots \ x_1\ x_0) = x_{m - 2}\ \ldots\ x_1\ x_0\ x_{m-1}$ en $log_2 k$ posiciones (hacia la izquierda). 
      - $\sigma^{k^{-1}}(x_{m-1}\ x_{m-2}\ \ldots \ x_1\ x_0) = x_0\ x_{m-1}\ x_{m - 2}\ \ldots\ x_1$ en $log_2k$ posiciones (hacia la derecha).
- *Código de compensación*, código para compensar una predicción que ha fallado.
 
